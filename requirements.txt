
fastapi>=0.111.0
uvicorn[standard]>=0.29.0
pdfplumber>=0.11.0
sentence-transformers>=3.0.0
optimum[onnxruntime]>=1.20.0
# If you have a CUDA GPU and want GPU-accelerated ONNX inference, replace the
# line above with:
#   optimum[onnxruntime-gpu]>=1.20.0
rank-bm25>=0.2.2
qdrant-client>=1.9.0
httpx>=0.27.0
cachetools>=5.3.0
numpy>=1.26.0
pydantic>=2.7.0